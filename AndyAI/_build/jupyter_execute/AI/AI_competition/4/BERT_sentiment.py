#!/usr/bin/env python
# coding: utf-8

# ### BERT可以使用预训练模型，比如先训练下百科全书

# adam优化器， adamw对参数区别对待，一些参数不做优化，效率更高

# In[ ]:


# pos_text, neg_text = readfile('./'), readfile('./')


# In[1]:


# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)


# Adam 优化器的原理：权重衰减，过拟合就加正则。

# In[ ]:




