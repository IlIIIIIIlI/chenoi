{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9b21fd",
   "metadata": {},
   "source": [
    "(Q)=\n",
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d139e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b637ca7",
   "metadata": {},
   "source": [
    "## 初始化GYM库的配置 - Initial gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "\n",
    "done = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e65aef",
   "metadata": {},
   "source": [
    "```{note}\n",
    "该游戏介绍：即操纵小车达到山坡的顶点\n",
    "Q-learning 策略：用得分点backpropagate到action的Q值，来选择action}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee550dd",
   "metadata": {},
   "source": [
    "## 对山地车做简单训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    action = 2\n",
    "    ## 每个环节都有三个参数， 状态， 奖励， 目标完成与否\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990aa106",
   "metadata": {},
   "source": [
    "````{tip} Q learning formulae\n",
    "\n",
    "```{image} images/20220717011435.png\n",
    ":name: label\n",
    "```\n",
    "\n",
    "we just pick up those with highest Q-value\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc31897",
   "metadata": {},
   "source": [
    "```{warning} 单说做循环，问题在什么地方？\n",
    "-> 由于每个动作的角度的state是continuous的，在这个问题中。Q-table可能会特别大，因此需要对Q-table做一个精简处理 （Discrete）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bd60a",
   "metadata": {},
   "source": [
    "## 对山地车state/ 动作的阈值做一个提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 获取所有动作中高点可能的值\n",
    "print(env.observation_space.high)\n",
    "## 获取所有动作中底部可能的值\n",
    "print(env.observation_space.low)\n",
    "## 获取动作个数  how many actions are possible\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586db6b",
   "metadata": {},
   "source": [
    "### 离散化 - 首先对于动作高点的范畴，我们将其变成20个模块 20 chunks\n",
    "** 这个 20 是自己定义的 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9d3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个个数为观测数的20的list\n",
    "discrete_size = [20] * len(env.observation_space.high)\n",
    "# 获取每段的长度\n",
    "discrete_win_size = (env.observation_space.high - env.observation_space.low) / discrete_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算state属于哪一个bucket\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state-env.observation_space.low) / discrete_win_size\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f0ff0",
   "metadata": {},
   "source": [
    "## 建立一个20*20 的q-table来储存state action对\n",
    "\n",
    "**该表是三维的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -2 和 0 靠直觉 和 经验 定义。。。\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(discrete_size+[env.action_space.n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7885d6ca",
   "metadata": {},
   "source": [
    "## 建立Q-leanring必需的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 学习率 0-1 \n",
    "learning_rate = 0.1\n",
    "# 用于折扣未来奖励对于现在步数的影响 how important is future reward\n",
    "discount = 0.95\n",
    "# episodes, 训练周期多少轮\n",
    "episodes= 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d960e0",
   "metadata": {
    "tags": [
     "margin"
    ]
   },
   "source": [
    "````{margin} np.max的用法：\n",
    "求序列的最值\n",
    "\n",
    "最少接收一个参数\n",
    "```\n",
    "> a = np.array([[0, 1, 6],\n",
    "                  [2, 4, 1]])\n",
    "> np.max(a)\n",
    "6\n",
    "> np.max(a, axis=0) # max of each column\n",
    "array([2, 4, 6])\n",
    "\n",
    "\n",
    "> b = np.array([3, 6, 1])\n",
    "> c = np.array([4, 2, 9])\n",
    "> np.maximum(b, c)\n",
    "array([4, 6, 9])\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc903590",
   "metadata": {},
   "source": [
    "## 构造一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 按照离散的进行训练\n",
    "discrete_state = get_discrete_state(env.reset())\n",
    "\n",
    "done=False\n",
    "while not done:\n",
    "    # 获取q表中，每个获得的离散状态下，返回q值最高的动作\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    # 做了这个工作之后进入下一个state\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    # 下一个state的离散形态\n",
    "    new_discrete_state = get_discrete_state(new_state)\n",
    "    env.render()\n",
    "    \n",
    "    # 如果没有做了该动作后没有达到target，那么下面使用Q-learning公式，根据公式我们知道，需要 1.旧Q值 2.学习率 3.奖励 4.折现率 5.对于未来估计的最大Q值\n",
    "    if not done:\n",
    "        # 我们先计算1和5\n",
    "        ## 做完动作后的Q值\n",
    "        current_Q = q_table[discrete_state + (action, )]\n",
    "        ## 在做完动作后的该状态下，未来最大的Q值\n",
    "        max_future_Q = np.max(q_table[new_discrete_state])\n",
    "        # 根据公式计算出新的Q值\n",
    "        new_Q = (1-learning_rate) * current_Q + learning_rate * (reward + discount * max_future_Q)\n",
    "        # 用新获得的Q值来更新Q表， key-value键值对，key是做完动作后的状态\n",
    "        q_table[discrete_state+(action, )] = new_Q\n",
    "    # 如果达到目标了，那么\n",
    "    elif new_state[0] >= env.goal_position:\n",
    "        # 在这次训练中，reward被设置为0，当达到目标时。。无惩罚项\n",
    "        q_table[discrete_state+(action,)] = 0\n",
    "    \n",
    "    # 更新状态到下一状态\n",
    "    discrete_state = new_discrete_state\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeef54",
   "metadata": {},
   "source": [
    "## 批次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefaeb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    if episode % 2000 == 0:\n",
    "        # 每2000轮，把这个小车加载出来看下\n",
    "        render=True\n",
    "        # 同时打印目前多少轮了\n",
    "    else:\n",
    "        render=False\n",
    "        \n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done=False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[discrete_state])\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if not done:\n",
    "            current_Q = q_table[discrete_state + (action, )]\n",
    "            max_future_Q = np.max(q_table[new_discrete_state])\n",
    "            new_Q = (1-learning_rate) * current_Q + learning_rate * (reward + discount * max_future_Q)\n",
    "            q_table[discrete_state+(action, )] = new_Q\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            # 当达到目的时，终止\n",
    "            print(f\"we made that in {episode}\")\n",
    "            q_table[discrete_state+(action,)] = 0\n",
    "            \n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c2186",
   "metadata": {},
   "source": [
    "结果：\n",
    "```{image} images/more-episodes-end-decay.png\n",
    ":name: label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147b614",
   "metadata": {},
   "source": [
    "````{tip} Epsilon\n",
    "在以上的训练中我们没有设置epsilon，这是一个随机值。用于balance exploration和exploitation\n",
    "\n",
    "- exploitation就是当找到一条正确道路的时候，继续往下走\n",
    "\n",
    "- exploration就是寻找其他随机道路，看是否能达到终点\n",
    "\n",
    "\n",
    "如果要加入Epsilon，那么\n",
    "```\n",
    "epsilon = 0.5\n",
    "start_epsilon_decay=1\n",
    "end_epsilon_decay=episodes//2\n",
    "epsilon_decay_value=epsilon/(end_epsilon_decay-start_epsilon_decay)\n",
    "```\n",
    "并在while not done同列的最后添加\n",
    "```\n",
    "if end_epsilon_decay >= episode >= start_epsilon_decay:\n",
    "    epsilon -= epsilon_decay_value\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fc1d1",
   "metadata": {},
   "source": [
    "## 小结 - 我们刚刚的训练只能保证达到target，但不能保证使用完美的策略最有效地达到target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ebff58",
   "metadata": {},
   "source": [
    "```{note}\n",
    "对于这个简单的项目来说，我们使用的Q-learning完全够了，但是对于复杂的环境，远远不止如此简单。下面我们开始进行Q-learning的高级一点的操作，比如可视化tracking。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64447ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 4000\n",
    "SHOW_EVERY = 20\n",
    "\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low)/DISCRETE_OS_SIZE\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2\n",
    "epsilon_decay_value = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "# 可视化代码片段插入1\n",
    "ep_rewards = []\n",
    "## 用于统计在当前episode，reward的情况（if better, how better; if worse, how worse the model is）\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(np.int))  # we use this tuple to look up the 3 Q values for the available actions in the q-table\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    # 可视化代码片段插入2, 定义单个episode的reward\n",
    "    ep_reward = 0\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(episode)\n",
    "    else:\n",
    "        render = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        # 可视化代码片段插入3， 当获得reward时加入\n",
    "        ep_reward += reward\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        \n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        #new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "        # If simulation did not end yet after last step - update Q table\n",
    "        if not done:\n",
    "\n",
    "            # Maximum possible Q value in next step (for new state)\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            # Current Q value (for current state and performed action)\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            # And here's our equation for a new Q value for current state and action\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            # Update Q table with new Q value\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        # Simulation ended (for any reson) - if goal position is achived - update Q value with reward directly\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            #q_table[discrete_state + (action,)] = reward\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Decaying is being done every episode if episode number is within decaying range\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "        \n",
    "    # 可视化代码片段插入4, 循环最外层的ep reward加上\n",
    "    ep_rewards.append(ep_reward)\n",
    "    \n",
    "     # 可视化代码片段插入5, 更新agg_reward\n",
    "    if not episode % SHOW_EVERY:\n",
    "        average_reward = sum(ep_rewards[-SHOW_EVERY:])/SHOW_EVERY\n",
    "        aggr_ep_rewards['ep'].append(episode)\n",
    "        aggr_ep_rewards['avg'].append(average_reward)\n",
    "        aggr_ep_rewards['max'].append(max(ep_rewards[-SHOW_EVERY:]))\n",
    "        aggr_ep_rewards['min'].append(min(ep_rewards[-SHOW_EVERY:]))\n",
    "        print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
    "    \n",
    "\n",
    "env.close()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
    "plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
    "# 图标的位置，lower right 右下角\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4f93e",
   "metadata": {},
   "source": [
    "````{tip} 对于想要保存Q table的，以下是方法\n",
    "```\n",
    "for episode in range(EPISODES):\n",
    "    ...\n",
    "    # AT THE END\n",
    "    np.save(f\"qtables/{episode}-qtable.npy\", q_table)\n",
    "\n",
    "env.close()\n",
    "```\n",
    "```{warning}\n",
    "当然要慎用以上代码，因为可能会保存很多很多个\n",
    "最好限制一下 -》\n",
    "\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "    \n",
    "    \n",
    "        np.save(f\"qtables/{episode}-qtable.npy\", q_table)\n",
    "        \n",
    "        \n",
    "        《-\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e8377",
   "metadata": {},
   "source": [
    "## 可视化一个Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "def get_q_color(value, vals):\n",
    "    if value == max(vals):\n",
    "        return \"green\", 1.0\n",
    "    else:\n",
    "        return \"red\", 0.3\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "ax1 = fig.add_subplot(311)\n",
    "ax2 = fig.add_subplot(312)\n",
    "ax3 = fig.add_subplot(313)\n",
    "\n",
    "i = 24999\n",
    "q_table = np.load(f\"qtables/{i}-qtable.npy\")\n",
    "\n",
    "\n",
    "for x, x_vals in enumerate(q_table):\n",
    "    for y, y_vals in enumerate(x_vals):\n",
    "        ax1.scatter(x, y, c=get_q_color(y_vals[0], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[0], y_vals)[1])\n",
    "        ax2.scatter(x, y, c=get_q_color(y_vals[1], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[1], y_vals)[1])\n",
    "        ax3.scatter(x, y, c=get_q_color(y_vals[2], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[2], y_vals)[1])\n",
    "\n",
    "        ax1.set_ylabel(\"Action 0\")\n",
    "        ax2.set_ylabel(\"Action 1\")\n",
    "        ax3.set_ylabel(\"Action 2\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f167f",
   "metadata": {},
   "source": [
    "````{margin} 该图的意义\n",
    "如果当前action的Q值是Max Q，那么就是绿色。视频40的代表为本代码20的观测维度。只是数字不同。\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f0fe9",
   "metadata": {},
   "source": [
    "结果：\n",
    "```{image} images/episode-25K-q-table.png\n",
    ":name: label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cd3ae",
   "metadata": {},
   "source": [
    "### 可视化全周期Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "def get_q_color(value, vals):\n",
    "    if value == max(vals):\n",
    "        return \"green\", 1.0\n",
    "    else:\n",
    "        return \"red\", 0.3\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "\n",
    "\n",
    "for i in range(0, 25000, 10):\n",
    "    print(i)\n",
    "    ax1 = fig.add_subplot(311)\n",
    "    ax2 = fig.add_subplot(312)\n",
    "    ax3 = fig.add_subplot(313)\n",
    "\n",
    "    q_table = np.load(f\"qtables/{i}-qtable.npy\")\n",
    "\n",
    "    for x, x_vals in enumerate(q_table):\n",
    "        for y, y_vals in enumerate(x_vals):\n",
    "            ax1.scatter(x, y, c=get_q_color(y_vals[0], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[0], y_vals)[1])\n",
    "            ax2.scatter(x, y, c=get_q_color(y_vals[1], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[1], y_vals)[1])\n",
    "            ax3.scatter(x, y, c=get_q_color(y_vals[2], y_vals)[0], marker=\"o\", alpha=get_q_color(y_vals[2], y_vals)[1])\n",
    "\n",
    "            ax1.set_ylabel(\"Action 0\")\n",
    "            ax2.set_ylabel(\"Action 1\")\n",
    "            ax3.set_ylabel(\"Action 2\")\n",
    "\n",
    "    #plt.show()\n",
    "    plt.savefig(f\"qtable_charts/{i}.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24338db8",
   "metadata": {},
   "source": [
    "### 并做成视频\n",
    "\n",
    "这个[离谱的视频。。](https://www.youtube.com/watch?v=ObMsyrwVXTc&t=2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2305e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def make_video():\n",
    "    # windows:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    # Linux:\n",
    "    #fourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "    out = cv2.VideoWriter('qlearn.avi', fourcc, 60.0, (1200, 900))\n",
    "\n",
    "    for i in range(0, 14000, 10):\n",
    "        img_path = f\"qtable_charts/{i}.png\"\n",
    "        print(img_path)\n",
    "        frame = cv2.imread(img_path)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "make_video()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
